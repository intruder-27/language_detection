{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_detection_bi-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpsyKVSiXnnp"
      },
      "source": [
        "**Using a Bi-LSTM Model for Language Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izWdvmzccgJE"
      },
      "source": [
        "**Drive Link (data, prediction and models) :** https://drive.google.com/drive/folders/1UWe1KH3Hyppc1U52b13k_v7P1uRwt16e?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7FS6l5HFEBW"
      },
      "source": [
        "**Dataset :** http://www.statmt.org/europarl/\n",
        "\n",
        "I cleaned the data, generated a csv file from each language corpus and then merged these csv files to create a single (multi-label) dataset, so that we can use it in the supervised training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE4Vrw-NE11c",
        "outputId": "71826ae6-be84-4393-b70f-bc30ca37bd68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPwDINSvX36P",
        "outputId": "b55531cf-730c-438c-f58a-6eab0e5145c8"
      },
      "source": [
        "%cd drive/MyDrive/lang_detect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1UWe1KH3Hyppc1U52b13k_v7P1uRwt16e/lang_detect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VyTcxeczcRS"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import re\n",
        "import json\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def word_tokenizer(data, senlen):\n",
        "    word_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "    word_tokenizer.fit_on_texts(data)\n",
        "    word_index = word_tokenizer.word_index\n",
        "    train_word_sequences = word_tokenizer.texts_to_sequences(data)\n",
        "    padded_word_sequences_dis = pad_sequences(train_word_sequences, padding='post', maxlen=senlen, truncating=\"post\")\n",
        "    return word_index, padded_word_sequences_dis\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def get_data(sentences, labels, label_index, batch_size):\n",
        "    sentence = torch.tensor(sentences, dtype=torch.long)\n",
        "    label = torch.tensor(labels, dtype=torch.long)\n",
        "    train_ds = TensorDataset(sentence, label)\n",
        "    train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "    return DeviceDataLoader(train_dl, get_default_device())\n",
        "\n",
        "def get_default_device():\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        # raise SystemError(\"CUDA NOT FOUND\")\n",
        "        return torch.device('cpu')\n",
        "        \n",
        "def read_txt(file_path):\n",
        "    sentence_list = []\n",
        "    label_list = [] \n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        sentence = \"\"\n",
        "        label = \"\"\n",
        "        for i in range(len(line)):\n",
        "          if(line[i] == \",\"):\n",
        "            label = line[0 : i].strip()\n",
        "            sentence = line[i+1 : ].strip()\n",
        "            # print(label , \" \", sentence)\n",
        "            break\n",
        "        sentence_list.append(sentence)\n",
        "        label_list.append(label)\n",
        "        \n",
        "      \n",
        "    return sentence_list, label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdqDbLrZcfby"
      },
      "source": [
        "def load_data(train_file_path, valid_file_path, test_file_path,batch_size=32):\n",
        "  train_sentences, train_labels = read_txt(train_file_path)\n",
        "  valid_sentences, valid_labels = read_txt(valid_file_path)\n",
        "  test_sentences, test_labels = read_txt(test_file_path)\n",
        "\n",
        "  train_len = len(train_sentences)\n",
        "  valid_len = len(valid_sentences)\n",
        "  test_len = len(test_sentences)\n",
        "\n",
        "  device = get_default_device()\n",
        "\n",
        "  sentences = train_sentences + valid_sentences + test_sentences\n",
        "  labels = train_labels + valid_labels + test_labels\n",
        "\n",
        "  max_sen_len = -1\n",
        "\n",
        "  for i in range(len(sentences)):\n",
        "    max_sen_len = max(max_sen_len , len(sentences[i].strip().split(\" \")))\n",
        "    sentences[i] = sentences[i].strip().split(\" \")\n",
        "    \n",
        "  # print(max_sen_len)\n",
        "  max_sen_len =min(max_sen_len,500)\n",
        "\n",
        "\n",
        "  word_index , tokenized_word_data = word_tokenizer(sentences, senlen=max_sen_len)\n",
        "  label_index, tokenized_label_data = word_tokenizer(labels , senlen=max_sen_len)\n",
        "\n",
        "  train_data_dl = get_data(tokenized_word_data[:train_len], tokenized_label_data[:train_len], label_index, batch_size)\n",
        "  valid_data_dl = get_data(tokenized_word_data[train_len:train_len+valid_len], tokenized_label_data[train_len:train_len+valid_len], label_index, batch_size)\n",
        "  test_data_dl = get_data(tokenized_word_data[-test_len:], tokenized_label_data[-test_len:], label_index, batch_size)\n",
        "  data = {'train_data_dl': train_data_dl, 'valid_data_dl': valid_data_dl, 'test_data_dl': test_data_dl, 'word_index': word_index,'label_index': label_index,'max_sen_len':max_sen_len}\n",
        "  \n",
        "  return data\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main_data = load_data(\"data/europarl.pp.train\" , \"data/europarl.pp.eval\",\"data/europarl_normalized.pp.test\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTFAzFQ-KBwO"
      },
      "source": [
        "#Base Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# import torchtext\n",
        "import torch\n",
        "# from torchtext.vocab import Vocab\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# import Data_preprocesser\n",
        "\n",
        "def get_default_device():\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        # raise SystemError(\"CUDA NOT FOUND\")\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list, tuple)):\n",
        "\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "       \n",
        "\n",
        "class Lang_Identify(nn.Module):\n",
        "    def __init__(self, vocab_size, max_sentence_len, input_size=300, word_embed_dim=300, label_size=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_embed_dim)\n",
        "        \n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size=128, bidirectional=True)\n",
        "        self.bilstm_2 = nn.LSTM(128 * 2, hidden_size=128, bidirectional=True)\n",
        "        self.lstm = nn.LSTM(128 * 2, hidden_size=128)\n",
        "        self.fc = nn.Linear(128, label_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input1):\n",
        "\n",
        "        self.sentence = input1\n",
        "        \n",
        "        we = self.word_embedding(self.sentence)  # (,123,300)\n",
        "        # print(\"Word Embedding : \", we.shape)\n",
        "    \n",
        "        lstm_1, (hidden, cell) = self.bilstm(we)\n",
        "        # print(\"LSTM_1 : \",lstm_1.shape)\n",
        "\n",
        "        lstm_2, (hidden, cell) = self.lstm(lstm_1)\n",
        "        # print(\"LSTM_2 : \",lstm_2.shape)\n",
        "\n",
        "        final_fc = self.fc(lstm_2)\n",
        "        # print(\"Final_FC \" ,final_fc.shape)\n",
        "        output = self.softmax(final_fc)\n",
        "        # print(\"Output : \",output.shape)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnIK8EYcWcHs"
      },
      "source": [
        "import numpy \n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def classification_report_org(y_true, y_pred, labels):\n",
        "\t'''Similar to the one in sklearn.metrics,\n",
        "\treports per classs recall, precision and F1 score'''\n",
        "\t# print(y_pred)\n",
        "\t# print(y_true)\n",
        "  \n",
        "\ty_true = numpy.asarray(y_true).ravel()\n",
        "\ty_pred = numpy.asarray(y_pred).ravel()\n",
        "\n",
        "\tcorrects = Counter(yt for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
        "\ty_true_counts = Counter(y_true)\n",
        "\ty_pred_counts = Counter(y_pred)\n",
        "\treport = ((lab,  # label\n",
        "\t\t\t   corrects[i] / max(1, y_true_counts[i]),  # recall\n",
        "\t\t\t   corrects[i] / max(1, y_pred_counts[i]),  # precision\n",
        "\t\t\t   y_true_counts[i]  # support\n",
        "\t\t\t   ) for i, lab in enumerate(labels))\n",
        "\treport = [(l, r, p, 2 * r * p / max(1e-9, r + p), s) for l, r, p, s in report]\n",
        "\n",
        "\tprint('{:<15}{:>10}{:>10}{:>10}{:>10}\\n'.format('',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t'recall',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t'precision',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t'f1-score',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t'support'))\n",
        "\tformatter = '{:<15}{:>10.4f}{:>10.4f}{:>10.4f}{:>10.4f}'.format\n",
        "\tfor r in report:\n",
        "\t\tprint(formatter(*r))\n",
        "\tprint('')\n",
        "\treport2 = list(zip(*[(r * s, p * s, f1 * s) for l, r, p, f1, s in report]))\n",
        "\tN = len(y_true)\n",
        "\tprint(formatter('avg / total',\n",
        "\t\t\t\t\tsum(report2[0]) / N,\n",
        "\t\t\t\t\tsum(report2[1]) / N,\n",
        "\t\t\t\t\tsum(report2[2]) / N, N) + '\\n')\n",
        "\tactual = Counter(y_true)\n",
        "\tdel actual[-1]\n",
        "\taccuracy = sum(corrects.values()) / sum(actual.values())\n",
        "\tprint('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m2HbL5CUbAb"
      },
      "source": [
        "# !pip install fastai --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMt6rMUM2c6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba45bdb4-8dcb-49fe-cc07-4a65306acfca"
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import json\n",
        "# import torchtext\n",
        "import torch\n",
        "# from torchtext.vocab import Vocab\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from fastai.test_utils import *\n",
        "from fastai.text.all import *\n",
        "\n",
        "\n",
        "data = main_data\n",
        "# Arguments and data\n",
        "train_data = data['train_data_dl']\n",
        "valid_data = data['valid_data_dl']\n",
        "test_data = data['test_data_dl']\n",
        "\n",
        "word_index = data['word_index']\n",
        "label_index = data['label_index']\n",
        "max_sen_len = data['max_sen_len']\n",
        "\n",
        "word_embedding_size = 300\n",
        "\n",
        "device = get_default_device()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"#################### Starting Training Model ####################\")\n",
        "\n",
        "\n",
        "model = Lang_Identify(vocab_size=len( word_index)+1,  max_sentence_len=max_sen_len, word_embed_dim = 300, label_size=len(label_index)+1)\n",
        "\n",
        "model = to_device(model, device)\n",
        "\n",
        "opt = torch.optim.Adam\n",
        "\n",
        "dls = DataLoaders(train_data, valid_data)\n",
        "learner = Learner(dls, model, loss_func=CrossEntropyLossFlat(flatten=True),metrics=[accuracy], lr=0.005, opt_func=Adam)\n",
        "# learner.fit(arg[\"epochs\"], lr = arg[\"lr\"] )\n",
        "\n",
        "\n",
        "# lr_min,lr_steep = learner.lr_find()\n",
        "# print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n",
        "\n",
        "learner.fit_one_cycle(10, lr_max = 0.005)\n",
        "\n",
        "\n",
        "print(\"----Ending Training-------- \\n Starting Validating Model:\")\n",
        "\n",
        "result_predic = []\n",
        "result_orig = []\n",
        "for *xb, yb in test_data:\n",
        "\n",
        "    preds = model(*xb)\n",
        "    preds = torch.argmax(preds, dim = -1)\n",
        "    result_predic.append(preds)\n",
        "    result_orig.append(yb)\n",
        "\n",
        "predic_f = torch.cat(result_predic).cpu()\n",
        "orig_f = torch.cat(result_orig).cpu()\n",
        "\n",
        "classification_report_org(orig_f, predic_f, label_index )\n",
        "print(\"#################### Saving ####################\")\n",
        "\n",
        "torch.save(model.state_dict(),\" bi-lstm_model/\")\n",
        "\n",
        "print(\"######################### End #########################\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.174304</td>\n",
              "      <td>3.174028</td>\n",
              "      <td>0.991781</td>\n",
              "      <td>35:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.174232</td>\n",
              "      <td>3.174012</td>\n",
              "      <td>0.993948</td>\n",
              "      <td>35:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.174252</td>\n",
              "      <td>3.173991</td>\n",
              "      <td>0.996563</td>\n",
              "      <td>35:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.174207</td>\n",
              "      <td>3.173995</td>\n",
              "      <td>0.995737</td>\n",
              "      <td>35:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.174287</td>\n",
              "      <td>3.174070</td>\n",
              "      <td>0.968439</td>\n",
              "      <td>35:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.174207</td>\n",
              "      <td>3.173987</td>\n",
              "      <td>0.995023</td>\n",
              "      <td>35:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.174199</td>\n",
              "      <td>3.173990</td>\n",
              "      <td>0.993714</td>\n",
              "      <td>35:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.174184</td>\n",
              "      <td>3.173987</td>\n",
              "      <td>0.994759</td>\n",
              "      <td>35:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.174258</td>\n",
              "      <td>3.173986</td>\n",
              "      <td>0.996451</td>\n",
              "      <td>35:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.174238</td>\n",
              "      <td>3.173986</td>\n",
              "      <td>0.997149</td>\n",
              "      <td>35:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Ending Training-------- \n",
            " Starting Validating Model:\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "<OOV>              0.9980    1.0000    0.999010372344.0000\n",
            "label              0.0000    0.0000    0.0000    0.0000\n",
            "en                 0.9985    0.9735    0.985820828.0000\n",
            "nl                 0.3570    0.0488    0.0858 1000.0000\n",
            "da                 0.0490    0.0469    0.0479 1000.0000\n",
            "sv                 0.0280    0.0504    0.0360 1000.0000\n",
            "pt                 0.0000    0.0000    0.0000 1000.0000\n",
            "es                 0.0460    0.0510    0.0484 1000.0000\n",
            "it                 0.3300    0.0448    0.0788 1000.0000\n",
            "fr                 0.0000    0.0000    0.0000 1000.0000\n",
            "de                 0.0760    0.0560    0.0645 1000.0000\n",
            "el                 0.0100    0.0391    0.0159 1000.0000\n",
            "bg                 0.0958    0.0522    0.0676  992.0000\n",
            "fi                 0.0000    0.0000    0.0000 1000.0000\n",
            "cs                 0.0020    0.0153    0.0035 1000.0000\n",
            "sl                 0.0000    0.0000    0.0000 1000.0000\n",
            "lt                 0.0000    0.0000    0.0000 1000.0000\n",
            "et                 0.0000    0.0000    0.0000 1000.0000\n",
            "lv                 0.0000    0.0000    0.0000 1000.0000\n",
            "ro                 0.0000    0.0000    0.0000  979.0000\n",
            "pl                 0.0000    0.0000    0.0000  928.0000\n",
            "sk                 0.0000    0.0000    0.0000 1000.0000\n",
            "hu                 0.0000    0.0000    0.0000  929.0000\n",
            "\n",
            "avg / total        0.9961    0.9980    0.997010414000.0000\n",
            "\n",
            "Accuracy: 0.9960656808142885\n",
            "#################### Saving ####################\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-04e7884d1c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#################### Saving ####################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" bi-lstm_model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######################### End #########################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: ' bi-lstm_model/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWhDIFs7z_a1"
      },
      "source": [
        "**I have used a 2 layer LSTM model in which the 1st layer ia a Bi-directional LSTM and the 2nd layer is a normal LSTM layer.\n",
        "Using the following model I obtain the follwing results on the validation set :**\n",
        "\n",
        "**1)Precision : 99.8%**\n",
        "\n",
        "**2)Recall    : 99.6%**\n",
        "\n",
        "**3)F1_Score  : 99.7%**\n",
        "\n",
        "**Due to resource constraints, I was able to train the model for 10 epochs only.\n",
        "If we train the model for more number of epochs and tune the hyperparameters properly, we'll definitely obtain better results.**\n",
        "\n",
        "**Moreover, due to resource constraints I kept the maximum sentence length, on which the model is trained, as 500 when the actual the maximum sentence length is 15062. Nevertheless, the model was able to learn the structural composition of the words and phrases as well the dependancies between the words and gave good results.**"
      ]
    }
  ]
}